{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apatite_XGBoost_ore_bearing\n",
    "By Yuyu Zheng       25/03/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, r2_score, make_scorer, f1_score, recall_score, precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Display the dataset\n",
    "df_raw = pd.read_excel(r\"Major and Trace dataset.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df_raw.copy()\n",
    "df.drop(df.columns[df.isna().mean() > 0.4], axis=1, inplace=True)  # Remove columns that >60% missing. \n",
    "df=df.drop(df.columns[0:1], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Class=df.Class.astype('category')\n",
    "df.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing Value proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sum() /(df.shape[0]* df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into Class and elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df.Class.values.copy()\n",
    "classes=classes.astype('str')\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "classes = le.fit_transform(classes)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(['Class'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, classes, test_size=0.2, \n",
    "                                                    stratify = classes, random_state = 2021) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(objective='binary:logistic', \n",
    "                    eval_metric = 'error',tree_method='hist', seed=2022)          \n",
    "xgb.fit(X_train, y_train)\n",
    "y_train_preds = xgb.predict(X_train)\n",
    "y_test_preds = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#display feature importance\n",
    "xgb.feature_importances_\n",
    "for feature_name, score in zip(list(features.columns), xgb.feature_importances_):\n",
    "    print(feature_name, \":\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(mod, X_train, X_test, y_train, y_test):\n",
    "    \"\"\" Returns a data frame of metrics (precision,\n",
    "        recall, AUC ROC) from training and test sets.\n",
    "        Assumes model has decision_function() method.\n",
    "        This will at least work for SVC, LDA, QDA.\n",
    "    \"\"\"\n",
    "    pred_train = mod.predict(X_train)\n",
    "    pred_test = mod.predict(X_test)\n",
    "    recall_train = recall_score(y_train, pred_train,average='weighted')\n",
    "    recall_test = recall_score(y_test, pred_test, average='weighted')\n",
    "    precision_train = precision_score(y_train, pred_train, average='weighted')\n",
    "    precision_test = precision_score(y_test, pred_test, average='weighted')\n",
    "    f1_train = f1_score(y_train, pred_train)\n",
    "    f1_test = f1_score(y_test, pred_test)\n",
    "    accuracy_train = accuracy_score(y_train, pred_train)\n",
    "    accuracy_test = accuracy_score(y_test, pred_test)\n",
    "    metrics = {'Set':['Train', 'Test'],\n",
    "               'Recall':[recall_train, recall_test],\n",
    "               'Precision':[precision_train, precision_test],\n",
    "               'f1':[f1_train, f1_test],\n",
    "               'Accuracy':[accuracy_train, accuracy_test]\n",
    "              }\n",
    "    return pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importance(feature_df, model, n_feature):\n",
    "    '''\n",
    "    Input:\n",
    "        feature_df: The feature dataframe / The X_train with column names\n",
    "        model: The training model\n",
    "        n_feature: number of feature importance you want to display\n",
    "    Output:\n",
    "        A plot with the top n feature importance in decreasing order\n",
    "    '''\n",
    "    vals = model.feature_importances_\n",
    "    df = pd.DataFrame(vals, index=feature_df.columns, columns=['Importance']).sort_values(by='Importance', ascending=False)\n",
    "    df_n = df.iloc[:n_feature,]\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    sns.barplot(df_n.index, df_n.Importance, palette=\"Set3\")\n",
    "    plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_metrics(xgb, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_importance(features, xgb, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3 ]\n",
    "depth = [3,4,5,6,7]\n",
    "min_split = np.linspace(0.1,2,20)\n",
    "alpha1 = [0.1,0.3,0.5,0.7,0.9, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(objective='binary:logistic', \n",
    "                    eval_metric = 'error', tree_method='hist', seed=2021,importance_type = 'cover')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cv = GridSearchCV(xgb, param_grid = {'eta': learning_rate, 'gamma': min_split, 'max_depth': depth, 'alpha':alpha1}, \n",
    "                      cv=5, scoring='f1') \n",
    "xgb_cv.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best = xgb_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = xgb_best.predict(X_train)\n",
    "y_test_preds = xgb_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_metrics(xgb_best, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = list(features.columns)\n",
    "importances = xgb_best.feature_importances_\n",
    "indices = np.argsort(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(range(len(indices)), importances[indices], color='c', align='center')\n",
    "plt.yticks(range(len(indices)), [features1[i] for i in indices],fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.xlabel('Relative Importance',fontsize=25)\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_test_preds)\n",
    "\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmat.shape[0]):\n",
    "    for j in range(confmat.shape[1]):\n",
    "        ax.text(x=j, y=i, s=confmat[i,j], va='center', ha='center',fontsize=20)\n",
    "plt.xlabel('predicted label',fontsize=20)\n",
    "plt.ylabel('true label',fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ============ The End =================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
